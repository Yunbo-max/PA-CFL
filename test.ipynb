{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.5-arm64-arm-64bit\n",
      "PyTorch Version: 2.1.0\n",
      "\n",
      "Python 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]\n",
      "Pandas 2.0.3\n",
      "Scikit-Learn 1.3.0\n",
      "GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ls/jfjq4hdn4nj9prylp62sg2780000gn/T/ipykernel_55741/1247987439.py:8: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  has_mps = getattr(torch,'has_mps',False)\n",
      "/var/folders/ls/jfjq4hdn4nj9prylp62sg2780000gn/T/ipykernel_55741/1247987439.py:9: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = \"mps\" if getattr(torch,'has_mps',False) \\\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0\n",
      "Torchvision version: 0.15.2\n",
      "Using Device:  mps\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 25020694.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 17492518.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 15815487.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 16755082.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310416\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.266019\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.207803\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.114183\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.968731\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.708174\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.295317\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.908230\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.804056\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.635014\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.574902\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.353327\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.411234\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.427622\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.271364\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.386618\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.401327\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.246344\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.247568\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.388606\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.226469\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.326761\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.306984\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.276141\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.158862\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.248900\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.247852\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.367755\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.241680\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.196972\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.229150\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.237323\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.402990\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.247053\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.181424\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.225744\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.203290\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.212318\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.162379\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.201457\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.162351\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.187994\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.226162\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.162772\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.086208\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.231328\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.108141\n",
      "\n",
      "Test set: Average loss: 0.1601, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.200510\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.100434\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.138896\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.169263\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.124273\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.119576\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.197097\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.082659\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.175565\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.094012\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.359313\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.135826\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.227976\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.103513\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.125398\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.193097\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.172182\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.238472\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.162255\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.188578\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.086625\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.153892\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.141564\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.075798\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.138463\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.070726\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.141583\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.157886\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.147157\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.171161\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.182386\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.190263\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.091220\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.060711\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.103454\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.152862\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.083071\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.086473\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.034028\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.134256\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.162767\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.114031\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.101743\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.079460\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.076338\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.103232\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.147964\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9708/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.061048\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.113525\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.111017\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.129138\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.084162\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.163006\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.086988\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.073435\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.141679\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.052219\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.103013\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.162990\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.078497\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.089172\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.102090\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.087379\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.039826\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.209559\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.087770\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.053106\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.177919\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.100632\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.076098\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.085265\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.121014\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.229174\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.052809\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.148174\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.054872\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.080348\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.058727\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.054242\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.121412\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.109560\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.044615\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.108381\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.125099\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.054739\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.084520\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.086136\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.075894\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.128079\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.059991\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.033953\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.042200\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.098999\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.113826\n",
      "\n",
      "Test set: Average loss: 0.0678, Accuracy: 9778/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.103557\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.108306\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.037054\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.059624\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.061925\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.058411\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.035108\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.079872\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.067757\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.107026\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.060904\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.153950\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.075794\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.023338\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.093626\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.037687\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.110488\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.045828\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.053282\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.066065\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.060626\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.098255\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.046811\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.106691\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.077134\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.048520\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.028481\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.086110\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.018704\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.072755\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.038790\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.063336\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.038527\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.057971\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.071008\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.080804\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.069691\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.069772\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.026861\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.074982\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.047351\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.079816\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.059249\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.037391\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.061962\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.035177\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.118451\n",
      "\n",
      "Test set: Average loss: 0.0683, Accuracy: 9783/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.089220\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.134495\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.037790\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.061906\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.030151\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.030764\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.090124\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.043172\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.036144\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.072793\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.030812\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.049420\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.041750\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.014323\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.024359\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.049772\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.046630\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.109513\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.088098\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.090960\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.042024\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.060635\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.073736\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.157983\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.044663\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.060272\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.027670\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.059931\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.126630\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.028247\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.057175\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.055811\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.011900\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.068365\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.022926\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.064245\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.060848\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.066696\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.049441\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.014930\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.066125\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.035431\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.046891\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.025084\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.085977\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.078741\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.056127\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9824/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST with PyTorch on Apple Silicon GPU\n",
    "\n",
    "Code borrowed from PyTorch Examples.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Device: \", device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=128, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import (\n",
    "    Ridge,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    ElasticNet,\n",
    "    Lasso,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_squared_error,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# Set the random seed \n",
    "torch.manual_seed(42)  \n",
    "\n",
    "# Define a custom dataset class\n",
    "class MarketDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.inputs[idx]\n",
    "        target = self.targets[idx]\n",
    "        return input_data, target\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_neurons, output_neurons, hidden_layers, neurons_per_layer, dropout\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_neurons, neurons_per_layer))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(neurons_per_layer, output_neurons))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_neurons)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Device: \", device)\n",
    "\n",
    "\n",
    "    # Set the number of clients, rounds, and epochs\n",
    "    sheet_name = [\n",
    "        \"0\",\n",
    "        \"1\",\n",
    "        \"2\",\n",
    "        \"3\",\n",
    "        \"5\",\n",
    "        \"6\",\n",
    "        \"7\",\n",
    "        \"9\",\n",
    "        \"10\",\n",
    "        \"12\",\n",
    "        \"14\",\n",
    "        \"16\",\n",
    "        \"17\",\n",
    "        \"22\",\n",
    "    ]\n",
    "\n",
    "    region_map = {\n",
    "        0: \"Southeast Asia\",\n",
    "        1: \"South Asia\",\n",
    "        2: \"Oceania\",\n",
    "        3: \"Eastern Asia\",\n",
    "        4: \"West Asia\",\n",
    "        5: \"West of USA\",\n",
    "        6: \"US Center\",\n",
    "        7: \"West Africa\",\n",
    "        8: \"Central Africa\",\n",
    "        9: \"North Africa\",\n",
    "        10: \"Western Europe\",\n",
    "        11: \"Northern Europe\",\n",
    "        12: \"Central America\",\n",
    "        13: \"Caribbean\",\n",
    "        14: \"South America\",\n",
    "        15: \"East Africa\",\n",
    "        16: \"Southern Europe\",\n",
    "        17: \"East of USA\",\n",
    "        18: \"Canada\",\n",
    "        19: \"Southern Africa\",\n",
    "        20: \"Central Asia\",\n",
    "        21: \"Eastern Europe\",\n",
    "        22: \"South of USA\",}\n",
    "\n",
    "\n",
    "    # Set the parameters for the model\n",
    "    input_neurons = 25\n",
    "    output_neurons = 1\n",
    "    hidden_layers = 4\n",
    "    neurons_per_layer = 64\n",
    "    dropout = 0.3\n",
    "\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(\n",
    "        \"market_data.h5\",\n",
    "        \"r\",\n",
    "    )\n",
    "\n",
    "    # Get the number of clients from sheet_name\n",
    "    num_clients = len(sheet_name)\n",
    "\n",
    "    # Set the number of iterations,rounds,epochs for federated learning\n",
    "    num_round = [i for i in range(3,100)]\n",
    "    num_epochs = 10\n",
    "    num_iterations = 10\n",
    "\n",
    "    # # Initialize an empty similarity matrix to store similarity values for each pair of clients\n",
    "    # similarity_matrix_total1 = np.zeros((len(sheet_name), len(sheet_name)))\n",
    "    # similarity_matrix_total2 = np.zeros((len(sheet_name), len(sheet_name)))\n",
    "    # similarity_matrix_total3 = np.zeros((len(sheet_name), len(sheet_name)))\n",
    "\n",
    "\n",
    "    # Initialize an empty similarity matrix to store similarity values for each pair of clients for each iteration\n",
    "    similarity_matrix_total = np.zeros((len(sheet_name), len(sheet_name), num_iterations))# Import statements and other code (excluding imports)...\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    # Preprocess the data\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    column_names = file[client].attrs[\"columns\"]\n",
    "    dataset.columns = column_names\n",
    "    dataset = dataset.drop(columns=[\"Region Index\"])\n",
    "\n",
    "    xs = dataset.drop([\"Sales\"], axis=1)\n",
    "    ys = dataset[\"Sales\"]\n",
    "\n",
    "    xs_train, xs_test, ys_train, ys_test = train_test_split(xs, ys, test_size=0.3, random_state=42)\n",
    "    xs_train, xs_val, ys_train, ys_val = train_test_split(xs_train, ys_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert data to tensors\n",
    "    train_inputs = torch.tensor(xs_train.values, dtype=torch.float32).to(device)\n",
    "    train_targets = torch.tensor(ys_train.values, dtype=torch.float32).to(device)\n",
    "    val_inputs = torch.tensor(xs_val.values, dtype=torch.float32).to(device)\n",
    "    val_targets = torch.tensor(ys_val.values, dtype=torch.float32).to(device)\n",
    "    test_inputs = torch.tensor(xs_test.values, dtype=torch.float32).to(device)\n",
    "    test_targets = torch.tensor(ys_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    return train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets\n",
    "\n",
    "def create_model(input_neurons, output_neurons, hidden_layers, neurons_per_layer, dropout):\n",
    "    # Define the neural network model\n",
    "    model = Net(input_neurons, output_neurons, hidden_layers, neurons_per_layer, dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "    epoch_loss = np.mean(train_losses)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            test_losses.append(loss.item())\n",
    "            test_preds.extend(outputs.cpu().numpy())  # Move predictions back to CPU for consistency\n",
    "            test_targets.extend(targets.cpu().numpy())  # Move targets back to CPU for consistency\n",
    "\n",
    "    r2 = r2_score(test_targets, test_preds)\n",
    "\n",
    "    return r2\n",
    "\n",
    "def main():\n",
    "    # Your existing code up to the point where you want to refactor...\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Device: \", device)\n",
    "\n",
    "    # Your existing code for loading data, setting parameters, and initializing global model...\n",
    "\n",
    "    for num_rounds in num_round:\n",
    "        print(f\"Testing {num_rounds}\")\n",
    "        \n",
    "        # Initialize a dictionary to store metrics for each client, round, and iteration\n",
    "        metrics = {\n",
    "            client: {\"r2\": [[[] for _ in range(num_rounds)] for _ in range(num_iterations)]}\n",
    "            for client in sheet_name\n",
    "        }\n",
    "\n",
    "        # Initialize a list to store the feature matrices for each iteration\n",
    "        all_feature_matrices = []\n",
    "\n",
    "        # Initialize a list to store the similarity matrices for each iteration\n",
    "        similarity_matrices = []\n",
    "\n",
    "                # Initialize a shared global model for this iteration\n",
    "        global_model, criterion, optimizer = create_model(input_neurons, output_neurons, hidden_layers, neurons_per_layer, dropout)\n",
    "\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "            # # Initialize a shared global model for this iteration\n",
    "            # global_model, criterion, optimizer = create_model(input_neurons, output_neurons, hidden_layers, neurons_per_layer, dropout)\n",
    "\n",
    "            # Initialize an empty list to store the client models for this round\n",
    "            client_models = []\n",
    "\n",
    "\n",
    "            for round in range(num_rounds):\n",
    "                print(f\"Round {round + 1}/{num_rounds}\")\n",
    "\n",
    "                for client in sheet_name:\n",
    "                    # Load the state dict of the global model to the client model\n",
    "\n",
    "                    train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets = preprocess_data(file[client][:])\n",
    "\n",
    "                    # Create data loaders\n",
    "                    train_dataset = MarketDataset(train_inputs, train_targets)\n",
    "                    val_dataset = MarketDataset(val_inputs, val_targets)\n",
    "                    test_dataset = MarketDataset(test_inputs, test_targets)\n",
    "                    train_loader = DataLoader(\n",
    "                        train_dataset, batch_size=32, shuffle=True\n",
    "                    )\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "                    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "                    # Training phase\n",
    "                    train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "                    \n",
    "                    # Use model to generate predictions for the test dataset\n",
    "                    client_models.append(model.state_dict())\n",
    "                    \n",
    "                    # Testing phase\n",
    "                    r2 = test_model(model, test_loader)\n",
    "                    # Save the R2 value for the current round and iteration\n",
    "                    metrics[client][\"r2\"][iteration][round] = r2\n",
    "\n",
    "                model.load_state_dict(global_model.state_dict())\n",
    "                # Average the weights across all clients after each round\n",
    "                averaged_weights = {\n",
    "                    k: sum(d[k] for d in client_models) / num_clients\n",
    "                    for k in client_models[0].keys()\n",
    "                }\n",
    "\n",
    "                # Update the global model\n",
    "                global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "            # Create the feature matrix for the current iteration and all rounds\n",
    "            feature_matrix = np.array(\n",
    "                [\n",
    "                    [metrics[client][\"r2\"][iteration][r] for r in range(num_rounds)]\n",
    "                    for client in sheet_name\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Check if the feature matrix is empty (no valid R2 values)\n",
    "            if feature_matrix.size == 0:\n",
    "                print(\"No valid data in the feature matrix. Skipping this iteration.\")\n",
    "                continue\n",
    "\n",
    "            # Append the feature matrix to the list after adding an additional dimension\n",
    "            all_feature_matrices.append(np.expand_dims(feature_matrix, axis=2))\n",
    "\n",
    "            # Concatenate the feature matrices along the third dimension to have shape (num_clients, num_rounds, num_iterations)\n",
    "            feature_matrix_total = np.concatenate(all_feature_matrices, axis=2)\n",
    "\n",
    "            # Step 2: Standardize the Data\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            # Flatten the last two dimensions\n",
    "            flattened_data = feature_matrix_total.reshape(\n",
    "                feature_matrix_total.shape[0], -1\n",
    "            )\n",
    "            normalized_data = scaler.fit_transform(flattened_data)\n",
    "\n",
    "            # Compute Pairwise Similarity using Sigmoid Kernel for the current iteration\n",
    "            similarity_matrix_total = sigmoid_kernel(normalized_data)\n",
    "            # print(similarity_matrix_total)\n",
    "\n",
    "            # Append the similarity matrix to the list of similarity matrices\n",
    "            similarity_matrices.append(similarity_matrix_total)\n",
    "\n",
    "                    \n",
    "\n",
    "            \n",
    "            train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets = preprocess_data(file[client][:])\n",
    "\n",
    "            # Training phase\n",
    "            train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "            \n",
    "            # Testing phase\n",
    "            r2 = test_model(model, test_loader)\n",
    "            # Save the R2 value for the current round and iteration\n",
    "            metrics[client][\"r2\"][iteration][round] = r2\n",
    "\n",
    "        # Save or use the R2 values as needed...\n",
    "\n",
    "# Rest of your existing code...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cambridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
